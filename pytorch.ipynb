{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial\n",
    "\n",
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/',train=True,transform=transforms.ToTensor(),download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',train=False,transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 输入1通道，输出10通道，kernel 5*5\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        # fully connect\n",
    "        self.fc = nn.Linear(320, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in_size = 64\n",
    "        in_size = x.size(0) # one batch\n",
    "        # x: 64*10*12*12\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        # x: 64*20*4*4\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        # x: 64*320\n",
    "        x = x.view(in_size, -1) # flatten the tensor\n",
    "        # x: 64*10\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangjinyi2000/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.317359\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.606252\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.450804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangjinyi2000/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/yangjinyi2000/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.3230, Accuracy: 8993/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.241951\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.198641\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.284072\n",
      "\n",
      "Test set: Average loss: 0.1770, Accuracy: 9468/10000 (95%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.131441\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.098740\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.173051\n",
      "\n",
      "Test set: Average loss: 0.1321, Accuracy: 9604/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.150855\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.256261\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.165370\n",
      "\n",
      "Test set: Average loss: 0.1086, Accuracy: 9687/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.145346\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.148958\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.132051\n",
      "\n",
      "Test set: Average loss: 0.0944, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.053519\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.121649\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.068839\n",
      "\n",
      "Test set: Average loss: 0.0786, Accuracy: 9752/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.084702\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.146826\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.106733\n",
      "\n",
      "Test set: Average loss: 0.0737, Accuracy: 9777/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.021796\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.120460\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.127400\n",
      "\n",
      "Test set: Average loss: 0.0705, Accuracy: 9770/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.123502\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.040197\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.050869\n",
      "\n",
      "Test set: Average loss: 0.0643, Accuracy: 9804/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def loss():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
